# ğŸ”„ Data Transformation Testing

This folder contains practical exercises focused on **transforming unstructured or messy data into clean, usable outputs** using prompt engineering.

The goal is to test how well large language models like **ChatGPT-4o**, **Gemini** and **Claude** can process and reformat data through structured, well-written prompts â€” turning raw inputs into summaries, tables, checklists, and more.

---

## ğŸ§ª Whatâ€™s Inside

Each test includes:
- A clearly defined **input** (e.g. feedback, notes, survey responses)
- A detailed **prompt instruction** with formatting and structure goals
- **Output comparisons** between ChatGPT, Gemini and Claude
- An evaluation of clarity, formatting, and data accuracy

---

## ğŸ” Prompt Use Cases Explored

- âœ… Rewriting messy lists into clean bullet points  
- âœ… Extracting action items from notes  
- âœ… Structuring survey responses into markdown tables  
- âœ… Summarizing feedback into key takeaways

---

## âš–ï¸ Why Compare Models?

Each exercise includes a **side-by-side comparison** of how ChatGPT, Gemini and Claude handle:
- Following detailed formatting instructions  
- Maintaining the meaning and tone of the original input  
- Producing clean, useful, and well-structured output  

This comparison helps me understand the strengths, limitations, and stylistic differences between different LLMs â€” and how to write better prompts for each.

---

## ğŸ¤ Open to Suggestions

If you're a prompt engineer, developer, or recruiter and have feedback on how to improve the format or structure of these tests, Iâ€™d love to hear from you.  
Feel free to fork, open an issue, or reach out.
