# 🔄 Data Transformation Testing

This folder contains practical exercises focused on **transforming unstructured or messy data into clean, usable outputs** using prompt engineering.

The goal is to test how well large language models like **ChatGPT-4o**, **Gemini** and **Claude** can process and reformat data through structured, well-written prompts — turning raw inputs into summaries, tables, checklists, and more.

---

## 🧪 What’s Inside

Each test includes:
- A clearly defined **input** (e.g. feedback, notes, survey responses)
- A detailed **prompt instruction** with formatting and structure goals
- **Output comparisons** between ChatGPT, Gemini and Claude
- An evaluation of clarity, formatting, and data accuracy

---

## 🔍 Prompt Use Cases Explored

- ✅ Rewriting messy lists into clean bullet points  
- ✅ Extracting action items from notes  
- ✅ Structuring survey responses into markdown tables  
- ✅ Summarizing feedback into key takeaways

---

## ⚖️ Why Compare Models?

Each exercise includes a **side-by-side comparison** of how ChatGPT, Gemini and Claude handle:
- Following detailed formatting instructions  
- Maintaining the meaning and tone of the original input  
- Producing clean, useful, and well-structured output  

This comparison helps me understand the strengths, limitations, and stylistic differences between different LLMs — and how to write better prompts for each.

---

## 🤝 Open to Suggestions

If you're a prompt engineer, developer, or recruiter and have feedback on how to improve the format or structure of these tests, I’d love to hear from you.  
Feel free to fork, open an issue, or reach out.
