# 🧠 Prompting Techniques Testing

This folder contains exercises focused on testing different **prompting strategies** with large language models like ChatGPT-4o.

Here I experiment with how the structure and phrasing of prompts affect the output. I compare the performance of methods such as **role prompting**, **few-shot prompting**, and **chain-of-thought reasoning** across different use cases.

Each test includes:

- A clearly defined prompt
- Output(s) from the model
- Notes on performance, clarity, and usefulness

---

## 🧪 Techniques Covered

- **Role Prompting**  
  Instructing the model to behave as a specific persona or expert.  
  _Example:_ “You are a sports psychologist. Explain how athletes mentally prepare for high-pressure competitions.”

- **Few-Shot Prompting**  
  Providing a few examples to teach the model the pattern or tone before it completes a new instance.  
  _Example:_  
  “Label the following movie reviews as Positive or Negative:

  - ‘I laughed the whole time.’ → Positive
  - ‘It was a waste of time.’ → Negative
  - ‘The pacing was strange...’ →”

- **Chain-of-Thought Prompting**  
  Asking the model to explain its reasoning step-by-step before giving the answer.  
  _Example:_ “If a train leaves at 3:15 PM and arrives at 4:45 PM, how long is the trip? Show your steps.”

---

These tests help me gain deeper control over how prompts guide the model’s behavior and logic.
