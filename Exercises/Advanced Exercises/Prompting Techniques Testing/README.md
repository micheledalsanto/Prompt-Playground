# ğŸ§  Prompting Techniques Testing

This folder contains exercises focused on testing different **prompting strategies** with large language models like ChatGPT-4o.

Here I experiment with how the structure and phrasing of prompts affect the output. I compare the performance of methods such as **role prompting**, **few-shot prompting**, and **chain-of-thought reasoning** across different use cases.

Each test includes:

- A clearly defined prompt
- Output(s) from the model
- Notes on performance, clarity, and usefulness

---

## ğŸ§ª Techniques Covered

- **Role Prompting**  
  Instructing the model to behave as a specific persona or expert.  
  _Example:_ â€œYou are a sports psychologist. Explain how athletes mentally prepare for high-pressure competitions.â€

- **Few-Shot Prompting**  
  Providing a few examples to teach the model the pattern or tone before it completes a new instance.  
  _Example:_  
  â€œLabel the following movie reviews as Positive or Negative:

  - â€˜I laughed the whole time.â€™ â†’ Positive
  - â€˜It was a waste of time.â€™ â†’ Negative
  - â€˜The pacing was strange...â€™ â†’â€

- **Chain-of-Thought Prompting**  
  Asking the model to explain its reasoning step-by-step before giving the answer.  
  _Example:_ â€œIf a train leaves at 3:15 PM and arrives at 4:45 PM, how long is the trip? Show your steps.â€

---

These tests help me gain deeper control over how prompts guide the modelâ€™s behavior and logic.
