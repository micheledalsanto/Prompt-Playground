# 🧪 Evaluation Prompting Exercises

This folder contains exercises focused on evaluating the quality of prompts, outputs, and reasoning steps generated by large language models (LLMs) like ChatGPT-4o, Gemini, and Claude.

The goal is to improve my ability as a **prompt engineer and evaluator**, learning to critique outputs based on structure, tone, completeness, clarity, and logic.

---

## 🧠 What’s Inside

Each exercise includes:
- A structured **evaluation prompt**
- A provided **input** (either a base prompt or a generated output)
- Responses from **multiple models**
- A **comparison table** to assess the models' evaluation skills
- Suggestions on how to improve either the original prompt or the model’s response

---

## 🧪 Evaluation Types Covered

- ✅ Output reviews (clarity, tone, completeness, accuracy)
- 🛠 Prompt improvement recommendations
- 🧵 Reasoning logic analysis
- 📋 Checklist creation for quality control

---

## ⚖️ Why Evaluation Prompting?

As LLMs become more integrated into workflows, it's not enough to generate good output — we need to:
- Critically evaluate what the model produces  
- Improve prompt quality iteratively  
- Build systems that **self-check** before shipping results  
- Compare how different models approach the same evaluation task

---

## 🤝 Open to Feedback

If you're a prompt engineer, ML researcher, or tech recruiter — I'm open to suggestions or feedback on how these evaluations could be improved or extended.  
Feel free to fork this folder and test the same evaluations with different models or configurations.
